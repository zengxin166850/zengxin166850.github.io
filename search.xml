<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[zookeeper一致性原理--ZAB协议(转)]]></title>
    <url>%2F2019%2F11%2F10%2Fzookeeper%E4%B8%80%E8%87%B4%E6%80%A7%E5%8E%9F%E7%90%86--ZAB%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[1.什么是ZAB协议，ZAB协议介绍 ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）。 Zookeeper 是一个为分布式应用提供高效且可靠的分布式协调服务。在解决分布式一致性方面，Zookeeper 并没有使用 Paxos ，而是采用了 ZAB 协议。 ZAB 协议定义：ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持 崩溃恢复 和 原子广播 协议。下面我们会重点讲这两个东西。 基于该协议，Zookeeper 实现了一种 主备模式 的系统架构来保持集群中各个副本之间数据一致性。具体如下图所示： 上图显示了 Zookeeper 如何处理集群中的数据。所有客户端写入数据都是写入到 主进程（称为 Leader）中，然后，由 Leader 复制到备份进程（称为 Follower）中。从而保证数据一致性。从设计上看，和 Raft 类似。 那么复制过程又是如何的呢？复制过程类似 2PC，ZAB 只需要 Follower 有一半以上返回 Ack 信息就可以执行提交，大大减小了同步阻塞。也提高了可用性。简单介绍完，开始重点介绍 消息广播 和 崩溃恢复。整个 Zookeeper 就是在这两个模式之间切换。 简而言之，当 Leader 服务可以正常使用，就进入消息广播模式，当 Leader 不可用时，则进入崩溃恢复模式。 2.消息广播ZAB 协议的消息广播过程使用的是一个原子广播协议，类似一个二阶段提交过程。对于客户端发送的写请求，全部由 Leader 接收，Leader 将请求封装成一个事务 Proposal，将其发送给所有 Follower ，然后，根据所有 Follower 的反馈，如果超过半数成功响应，则执行 commit 操作（先提交自己，再发送 commit 给所有 Follwer）。 基本上，整个广播流程分为 3 步骤： 1.将数据都复制到 Follwer 中 2.等待 Follwer 回应 Ack，最低超过半数即成功 3.当超过半数成功回应，则执行 commit ，同时提交自己 通过以上 3 个步骤，就能够保持集群之间数据的一致性。实际上，在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，避免同步，实现异步解耦。 还有一些细节： Leader 在收到客户端请求之后，会将这个请求封装成一个事务，并给这个事务分配一个全局递增的唯一 ID，称为事务ID（ZXID），ZAB 协议需要保证事务的顺序，因此必须将每一个事务按照 ZXID 进行先后排序然后处理。 在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞。 zookeeper集群中为保证任何所有进程能够有序的顺序执行，只能是 Leader 服务器接受写请求，即使是 Follower 服务器接受到客户端的请求，也会转发到 Leader 服务器进行处理。 实际上，这是一种简化版本的 2PC，不能解决单点问题。等会我们会讲述 ZAB 如何解决单点问题（即 Leader 崩溃问题）。 3.崩溃恢复刚刚我们说消息广播过程中，Leader 崩溃怎么办？还能保证数据一致吗？如果 Leader 先本地提交了，然后 commit 请求没有发送出去，怎么办？ 实际上，当 Leader 崩溃，即进入我们开头所说的崩溃恢复模式（崩溃即：Leader 失去与过半 Follwer 的联系）。下面来详细讲述。 假设1：Leader 在复制数据给所有 Follwer 之后崩溃，怎么办？假设2：Leader 在收到 Ack 并提交了自己，同时发送了部分 commit 出去之后崩溃怎么办？ 针对这些问题，ZAB 定义了 2 个原则： ZAB 协议确保那些已经在 Leader 提交的事务最终会被所有服务器提交。ZAB 协议确保丢弃那些只在 Leader 提出/复制，但没有提交的事务。所以，ZAB 设计了下面这样一个选举算法：能够确保提交已经被 Leader 提交的事务，同时丢弃已经被跳过的事务。 针对这个要求，如果让 Leader 选举算法能够保证新选举出来的 Leader 服务器拥有集群总所有机器编号（即 ZXID 最大）的事务，那么就能够保证这个新选举出来的 Leader 一定具有所有已经提交的提案。而且这么做有一个好处是：可以省去 Leader 服务器检查事务的提交和丢弃工作的这一步操作。 这样，我们刚刚假设的两个问题便能够解决。假设 1 最终会丢弃调用没有提交的数据，假设 2 最终会同步所有服务器的数据。这个时候，就引出了一个问题，如何同步？ 4.数据同步当崩溃恢复之后，需要在正式工作之前（接收客户端请求），Leader 服务器首先确认事务是否都已经被过半的 Follwer 提交了，即是否完成了数据同步。目的是为了保持数据一致。 当所有的 Follwer 服务器都成功同步之后，Leader 会将这些服务器加入到可用服务器列表中。 实际上，Leader 服务器处理或丢弃事务都是依赖着 ZXID 的，那么这个 ZXID 如何生成呢？ 答：在 ZAB 协议的事务编号 ZXID 设计中，ZXID 是一个 64 位的数字，其中低 32 位可以看作是一个简单的递增的计数器，针对客户端的每一个事务请求，Leader 都会产生一个新的事务 Proposal 并对该计数器进行 + 1 操作。 而高 32 位则代表了 Leader 服务器上取出本地日志中最大事务 Proposal 的 ZXID，并从该 ZXID 中解析出对应的 epoch 值，然后再对这个值加一。 高 32 位代表了每代 Leader 的唯一性，低 32 代表了每代 Leader 中事务的唯一性。同时，也能让 Follwer 通过高 32 位识别不同的 Leader。简化了数据恢复流程。 基于这样的策略：当 Follower 链接上 Leader 之后，Leader 服务器会根据自己服务器上最后被提交的 ZXID 和 Follower 上的 ZXID 进行比对，比对结果要么回滚，要么和 Leader 同步。 原文链接]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走近分布式协调服务--zookeeper]]></title>
    <url>%2F2019%2F11%2F03%2F%E8%B5%B0%E8%BF%91%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E6%9C%8D%E5%8A%A1--zookeeper%2F</url>
    <content type="text"><![CDATA[分布式环境的特点并发性程序运行过程中，并发性操作是很常见的。比如同一个分布式系统中的多个节点，同时访问一个共享资源、数据库等。 分布性 无序性 分布式环境下面临的问题网络通信网络本身的不可靠性，因此会涉及到一些网络通信问题。 分区容错当网络发生异常导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式架构的所有节点，只有部分节点能够正常通信。 CAP理论C一致性：所有节点上的数据时刻保持一致。 A可用性：每次请求都能收到响应，不管结果成功或失败。 P分区容错性。cap介绍 由于这三个指标不可能同时做到,在分区容错性是必须的情况下，只能选择取舍CP或AP其中一种。 12(CAP理论仅适用于原子读写的Nosql场景，不适用于数据库系统？因为如果出现更新失败导致数据丢失的情况，无法恢复) zookeeper是CP理论的一种典型。 BASE 理论ebay提出的BASE理论，放宽了对事务的ACID要求。保证基本可用。软状态。保证数据的最终一致性。 (兼顾可用性和一致性) Basically available ：基本可用。正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1～2秒。 soft-state： 软状态。即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 Eventually Consistent：最终一致性。 zookeeper是什么zookeeper是一个开源的分布式协调服务，是由雅虎创建的，基于google chubby。 是分布式数据一致性的一种解决方案。 zookeeper能做什么数据的发布/订阅（配置中心:disconf） 、 负载均衡（dubbo利用了zookeeper机制实现负载均衡） 、命名服务、master选举(kafka、hadoop、hbase)、分布式队列、分布式锁。 顺序一致性：从同一个客户端发起的事务请求，最终会严格按照顺序被应用到zookeeper中 原子性：所有的事务请求的处理结果在整个集群中的所有机器上的应用情况是一致的，也就是说，要么整个集群中的所有机器都成功应用了某一事务、要么全都不应用。 可靠性：一旦服务器成功应用了某一个事务数据，并且对客户端做了响应，那么这个数据在整个集群中一定是同步并且保留下来的。 实时性：一旦一个事务被成功应用，客户端就能够立即从服务器端读取到事务变更后的最新数据状态；（zookeeper仅仅保证在一定时间内，近实时） 一些概念zookeeper的数据模型和文件系统类似，每一个节点称为：znode. 是zookeeper中的最小数据单元。每一个znode上都可以保存数据和挂载子节点。从而构成一个层次化的属性结构。 节点特性 持久化节点 ：节点创建后会一直存在zookeeper服务器上，直到主动删除。 持久化有序节点：每个节点都会为它的一级子节点维护一个顺序。 临时节点 ：临时节点的生命周期和客户端的会话保持一致。当客户端会话失效，该节点自动清理。 临时有序节点：在临时节点上多勒一个顺序性特性。 zookeeper集群zookeeper集群, 包含三种角色: leader / follower /observer。 1234567#在datadir添加一个myid，配置server.myid=ip:port1:port2#添加observer节点的时候需要在zoo.cfg中增加peerType=observerserver.0=192.168.11.128:2888:3888server.1=192.168.11.129:2888:3888server.2=192.168.11.130:2888:3888server.3=192.168.11.131:2888:3888:observer 其中，3888用于leader选举。2888为节点间通信端口。 observer节点比较特殊，用于提升zookeeper集群的扩展性。因为随着连接的client增多，server的集群也必须扩大，而zk集群选举需要半数以上机器投票通过，代价较大。observer不参与投票只接收投票结果。可以做到在不影响写性能的情况下去扩展zk。 zookeeper配置文件1234567891011tickTime=2000 zookeeper中最小的时间单位长度 （ms）initLimit=10 follower节点启动后与leader节点完成数据同步的时间syncLimit=5 leader节点和follower节点进行心跳检测的最大延时时间dataDir=/tmp/zookeeper 表示zookeeper服务器存储快照文件的目录dataLogDir 表示配置 zookeeper事务日志的存储路径，默认指定在dataDir目录下clientPort 表示客户端和服务端建立连接的端口号： 2181 zookeeper的客户端使用123456789101112131415161718#使用help查看命令 1. create [-s] [-e] path data acl-s 表示节点是否有序-e 表示是否为临时节点默认情况下，是持久化节点，且无序2. get path [watch]获得指定 path的信息3.set path data [version]修改节点 path对应的data4.delete path [version]删除节点5.deleteall path delete不能删除有子节点的节点。deleteall可以(rmr命令已过时)。 Watcher zookeeper提供了分布式数据发布/订阅,zookeeper允许客户端向服务器注册一个watcher监听。当服务器端的节点触发指定事件的时候会触发watcher。服务端会向客户端发送一个事件通知 watcher的通知是一次性，一旦触发一次通知后，该watcher就失效. ACL zookeeper提供控制节点访问权限的功能，用于有效的保证zookeeper中数据的安全性。避免误操作而导致系统出现重大事故。CREATE/READ/WRITE/DELETE/ADMIN ZooKeeper 的权限管理通过Server、Client 两端协调完成： 1234567891011121314151617181920212223242526272829(1) Server端一个ZooKeeper 的节点存储两部分内容：数据和状态，状态中包含ACL 信息。创建一个znode 会产生一个ACL 列表，列表中每个ACL 包括：① 权限perms② 验证模式schema③ 具体内容expression：Ids例如，当scheme=&quot;digest&quot; 时， Ids 为用户名密码， 即&quot;root ：J0sTy9BCUKubtK1y8pkbL7qoxSw&quot;。ZooKeeper 提供了如下几种验证模式：① Digest： Client 端由用户名和密码验证，譬如user:pwd② Host： Client 端由主机名验证，譬如localhost③ Ip：Client 端由IP 地址验证，譬如172.2.0.0/24④ World ：固定用户为anyone，为所有Client 端开放权限当会话建立的时候，客户端将会进行自我验证。权限许可集合如下：① Create 允许对子节点Create 操作② Read 允许对本节点GetChildren 和GetData 操作③ Write 允许对本节点SetData 操作④ Delete 允许对子节点Delete 操作⑤ Admin 允许对本节点setAcl 操作 javaapi 原生api zkclient curator api的使用细节之后详细记录 实现数据发布订阅/ 配置中心实现配置信息的集中式管理和数据的动态更新 实现配置中心有两种模式：push 、pull。(文件的推和拉) zookeeper采用的是推拉相结合的方式。 客户端向服务器端注册自己需要关注的节点。一旦节点数据发生变化，那么服务器端就会向客户端发送watcher事件通知。客户端收到通知后，主动到服务器端获取更新后的数据。 适用情况特征： 数据量比较小 数据内容在运行时会发生动态变更 集群中的各个机器共享配置 实现集群管理实现集群管理，首先所有服务器在启动时向zookeeper中指定节点下，注册自己(创建临时节点)。基于临时节点和watcher机制的特性，当有节点创建成功时，会发送一个通知节点上线。节点意外宕机时，也会发出通知。 实现分布式锁分布式锁的原理也是同样，当服务想要获取锁时，在zookeeper中有指定的持久节点，需要获取锁的线程需要到改节点下去创建临时有序节点，这样在zk中，节点创建成功，并且在所有子节点中排序为最小的那个线程成功获得锁。没有拿到锁的线程改为监控比自己节点小1的节点，当任务执行完毕，临时节点会被删除，此时触发watcher机制。通知下一个线程获取锁。 实现独占锁的机制也类似，所有节点都去创建子节点，创建成功者获得锁，其余等待。 实现分布式队列分布式队列可以在zk中维护一个持久节点，节点下存一个data值为队列的容纳数量。当程序入队时，在该节点下创建子节点，当子节点个数达到父节点下的data设置个数时，表示队列满。无节点时表示队列空。每当出队时删除对应的节点。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes集群组件介绍]]></title>
    <url>%2F2019%2F07%2F20%2Fkubernetes%E9%9B%86%E7%BE%A4%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[一个Kubernetes集群，通常是由多个工作节点Worker Node加上一个集群控制者Master，以及集群状态存储系统etcd组成。 系统组件图 Master NodeMaster节点主要由负责API服务的apiserver、负责容器编排的controller-manager、以及负责调度的scheduler组成。整个集群的持久化数据，由apiserver处理后，交由etcd进行管理。 API server负责Restful风格的kubernetes API，负责接收、响应、校验发入集群的所有REST请求。 Controller Manager 负责维护集群的状态，包括很多资源的控制器，是保证 Kubernetes 声明式 API 工作的大脑，比如故障检测、自动扩展、滚动更新等。 Scheduler负责管理集群内各节点的资源状态。创建Pod时，根据合适的资源做出调度决策。在创建Pod的Events中可以看到，第一个Event便是Schedule。 12345678910kubectl describe pod image-sign-deployment-59948dd446-8n8l7...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 108s default-scheduler Successfully assigned default/image-sign-deployment-59948dd446-8n8l7 to node2 Normal Pulled 107s kubelet, node2 Container image &quot;image-sign&quot; already present on machine Normal Created 107s kubelet, node2 Created container image-sign Normal Started 106s kubelet, node2 Started container image-sign Worker Node在worker Node中，最重要的则是kubelet，它所负责的是与容器运行时打交道（如docker项目）。他们交互所依赖的，是一个称作CRI的远程调用接口，这个接口定义了容器运行时的各项核心操作。如启动一个容器所需的所有参数。 也正是因为CRI的存在，kubernetes并不关系用户部署的容器运行时是Docker亦或是其它的（但默认的是Docker），只要这个容器运行时能运行标准的镜像，它就能通过实现CRI接入到Kubernetes中。 具体的容器运行时，像Docker则一般通过OCI这个容器运行时规范，与底层的Linux操作系统进行交互。即：将CRI请求翻译成对Linux系统的调用，操作系统的NameSpace和CGroups等。 此外，kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于 Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能。 kubelet 的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储。这两个插件与 kubelet 进行交互的接口，分别是 CNI（Container Networking Interface）和 CSI（Container Storage Interface）。 而kube-proxy，它所负责的是能够按需为Service资源对象创建iptables或ipvs规则。从而捕获访问当前访问Service的请求，将其转发至正确的Pod对象 核心附件kubernetes还需要依赖一组称为“附件”的组件来提供完整的功能。这些组件通常由第三方提供，运行在kubernetes集群上。 CoreDNS 在Kubernetes集群中调度运行提供DNS服务的Pod，同一集群中的其他Pod可使用此DNS服务实现service name到 cluster IP的解析 。DNS服务是Kubernetes赖以实现服务发现的核心组件之一，默认情况下只会创建一个DNS Pod，在生产环境中我们需要对coredns进行扩容。 有两种方式： 12手动扩容 kubectl –namespace=kube-system scale deployment coredns –replicas=使用 DNS Horizontal Autoscaler Kubernetes Dashboard： 一个Web UI,用于管理kubernetes集群中应用，以及集群自身。 Ingress Controller： 暂时没学到相关内容。之后再来补充。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>容器</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一个kubernetes应用]]></title>
    <url>%2F2019%2F07%2F20%2F%E7%AC%AC%E4%B8%80%E4%B8%AAKubernetes%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[使用小细节从节点上使用kubectl时，如果出现 1The connection to the server localhost:8080 was refused - did you specify the right host or port? 可以使用以下命令解决： 1export KUBECONFIG=/etc/kubernetes/admin.conf 原因暂时母鸡，我试过把这句话加到环境变量，但是并没有卵用…..（问题找到了。。将master节点的/etc/kubernetes/admin.conf拷贝到 worker节点上）执行如下即可： 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 另外，每次集群宕机后，各节点都需要运行一遍swapoff -a ，不然无法启动。 第一个kubernetes应用在上一篇笔记中，已经使用kubeadm这个工具搭建好了一个完整的kubernetes集群，现在就是时候来体验一下如何使用kubernetes管理容器应用了。 首先，kubernetes是基于容器进行部署的，那么我们需要的第一个东西就是镜像。然后，我们需要根据kubernetes的规范和要求，将镜像组织为kubernetes能够认识的方式： 编写yaml文件。 kubernetes与Docker的不同之处就在于，kubernetes不推荐使用命令行的方式来运行容器，而是使用yaml/json的方式来运行。将容器的信息都记录在文件中，然后使用如下的语句将它运行起来： 1kubectl create -f xxx.yaml 这样，在运行容器的同时，也记录下了run的相关信息。在需要变更时，可以有相关记录。（其实在容器编排上来说docker-compose也是这样的思路，只是kubernetes提供的功能相对来说更加的强大和全面。）比如下面这个yaml文件：12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.16.0 ports: - containerPort: 80 在这个文件中，metadata代表的是元数据信息，name：nginx-deployment就是为这个Deployment自定义的名称，而kind: Deployment代表的是定义多个副本的应用。replicas告诉kubernetes需要在所有节点中，保证有且仅有2个节点在同时运行。 除此之外，template定义了pod的模板，声明每个创建出来的容器都带有一个标签app：nginx，使用的基础镜像为nginx:1.16.0，并且开放容器端口80。 使用kubectl apply指令创建/更新这个yaml文件对应的容器： 1kubectl apply -f nginx-deployment.yaml kubectl create 和 kubectl replace 分别对应着创建和更新，但一般不推荐，因为apply可以统一完成这两种操作。 接下来，可以通过kubectl get 检查运行结果：123456kubectl get pods -l app=nginx...NAME READY STATUS RESTARTS AGEnginx-deployment-75fdccc955-vpdqm 1/1 Running 0 25snginx-deployment-75fdccc955-wdfxf 1/1 Running 0 25s 这个命令中，-l表示的是查找pod中label标签为app=nginx的，也就是yaml文件中所配置的那样，可以看到两个副本都已经处于running状态，表示这个Deployment所管理的Pod都处于预期状态。 此外，使用kubectl describe可以查看pod的相关信息。 123456789101112131415161718192021kubectl describe pod nginx-deployment-75fdccc955-vpdqm...Name: nginx-deployment-75fdccc955-vpdqmNamespace: defaultNode: node1/192.168.110.130Labels: app=nginx pod-template-hash=75fdccc955Status: RunningIP: 10.244.0.54Controlled By: ReplicaSet/nginx-deployment-75fdccc955............Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 65s default-scheduler Successfully assigned default/nginx-deployment-75fdccc955-vpdqm to node1 Normal Pulling 64s kubelet, node1 Pulling image &quot;nginx:1.16.0&quot; Normal Pulled 44s kubelet, node1 Successfully pulled image &quot;nginx:1.16.0&quot; Normal Created 44s kubelet, node1 Created container nginx Normal Started 43s kubelet, node1 Started container nginx 在返回结果中，可以看到诸如name、node、labels、IP等等一系列相关信息。除此之外，还有一个特别的部分：Events。在Events中，可以看到这个Pod从分配节点到拉取镜像再到运行容器的整个流程。所以，当运行发生异常时往往可以在这里看到错误信息，帮助Debug。 当需要进行升级时，如从1.16.0升级到latest版本，只需要将yaml文件对应的镜像改为image: nginx即可，然后运行 1234kubectl apply -f nginx-deployment.yaml...deployment.apps/nginx-deployment configured 如果速度较快的话，可以查看到如下情况：（停止和删除原先的Pod，并重新创建的过程） 12345678kubectl get pods -l app=nginx...NAME READY STATUS RESTARTS AGEnginx-deployment-75fdccc955-vpdqm 1/1 Terminating 0 31mnginx-deployment-75fdccc955-wdfxf 1/1 Running 0 31mnginx-deployment-7bffbd4747-kz8qt 0/1 ContainerCreating 0 1snginx-deployment-7bffbd4747-qb9x2 1/1 Running 0 2s 接下来，尝试为容器添加Volume数据卷映射，修改yaml内容为如下所示：123456789101112131415161718192021222324252627apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; name: nginx-vol volumes: - name: nginx-vol hostPath: path: /var/data 可以看到，在这个yaml中增加了一个volumeMounts的属性，它所代表的含义是将容器mountPath所指定的目录挂载到名称为nginx-vol的Volume中，同时volumes属性声明了一个名为nginx-vol的卷供容器使用，并且类型为hostPath。这样一来，宿主机的/var/data目录就和容器的mountPath目录建立了映射关系。 运行kubectl describe指令可以查看到映射的信息： 123456789101112131415161718192021222324............Containers: nginx: Container ID: docker://db553728961f2b62bb3fc48c46e2053674743d04eab21abd19528ae28ad9abde Image: nginx:1.16.0 Image ID: docker-pullable://nginx@sha256:71f04b5caf2f921f4f0752b036be5a2d005f22c10e946fde6b2aa22676579d66 Port: 80/TCP Host Port: 0/TCP State: Running Started: Sat, 20 Jul 2019 13:46:26 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /usr/share/nginx/html from nginx-vol (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-6nvp9 (ro)......Volumes: nginx-vol: Type: HostPath (bare host directory volume) Path: /var/data HostPathType: ...... 到这里，还可以使用kubectl exec指令进入到Pod中 1234kubectl exec -it nginx-deployment-7bffbd4747-kz8qt sh...# ls /usr/nginx/share/html 最后，如果需要从集群中删除这个nginx-deployment的话，执行kubectl delete指令即可 1234kubectl delete -f nginx-deployment.yaml...deployment.apps &quot;nginx-deployment&quot; deleted 制作image-sign的Deployment在有了上述知识后，我决定将之前在alpine镜像安装字体库使用到的image-sign制作为一个deployment。（可以使用docker pull hoppou/image-sign 从dockerhub拉取这个镜像呢，另外记得改镜像名称~）编写如下yaml： 1234567891011121314151617181920apiVersion: apps/v1kind: Deploymentmetadata: name: image-sign-deploymentspec: selector: matchLabels: app: image-sign replicas: 2 template: metadata: labels: app: image-sign spec: containers: - name: image-sign image: image-sign imagePullPolicy: IfNotPresent ports: - containerPort: 9466 由于镜像功能极为简单，yaml的内容也很清晰，只需要将端口打开即可，运行: 123kubectl apply -f image-sign-deployment.yaml ...deployment.apps/image-sign-deployment created 1234kubectl get pod -l app=image-signNAME READY STATUS RESTARTS AGEimage-sign-deployment-59948dd446-8n8l7 1/1 Running 0 16simage-sign-deployment-59948dd446-fsbdj 1/1 Running 0 16s 查看其中一个pod的相关属性： 123456789101112131415kubectl describe pod image-sign-deployment-59948dd446-8n8l7...Node: node2/192.168.110.131Status: RunningIP: 10.244.1.40Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 108s default-scheduler Successfully assigned default/image-sign-deployment-59948dd446-8n8l7 to node2 Normal Pulled 107s kubelet, node2 Container image &quot;image-sign&quot; already present on machine Normal Created 107s kubelet, node2 Created container image-sign Normal Started 106s kubelet, node2 Started container image-sign 可以看到IP信息为10.244.1.40，接下来使用postman进行测试： 大功告成！！！ヽ(・∀・)ノTips补充在kubernetes中拉取镜像有如下三种策略，可以通过imagePullPolicy进行设置 123Never ---- 从不拉取，只使用已有镜像Always ---- 总是拉取 IfNotPresent ---- 当本地不存在时拉取 目前版本的kubernetes默认使用的是IfNotPresent，但在没有标明镜像的tag时，如 image: nginx 这种写法会采用Always的方式进行拉取。]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>容器</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用kubeadm安装kubernetes 1.15版本]]></title>
    <url>%2F2019%2F07%2F14%2F%E4%BD%BF%E7%94%A8kubeadm%E5%AE%89%E8%A3%85kubernetes1.15%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[准备1.1 系统配置准备两台机器，关闭防火墙(这里列举的node1为192.168.110.130，node2为131)，123cat /etc/hosts192.168.110.130 node1192.168.110.131 node2 systemctl stop firewalld 或按照https://kubernetes.io/docs/setup/independent/install-kubeadm/说明，开放端口。 关闭selinux，然后reboot重启（selinux是 Linux历史上最杰出的新安全子系统） 123setenforce 0 ---- 0关闭，1启用vi /etc/selinux/configSELINUX=disabled 创建/etc/sysctl.d/k8s.conf文件，添加如下内容： 123net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1 执行命令使修改生效。 12modprobe br_netfilter ----该指令用于向内核中加载/移除模块sysctl -p /etc/sysctl.d/k8s.conf ----从指定文件加载系统参数 1.2 kube-proxy开启ipvs的前置条件kube-proxy是kubernetes重要的组件，它的作用是虚拟出一个VIP，保证VIP无论后台服务（pod，Endpoint）如何变更都保持不变，起到一个负载均衡的功能。kube-proxty有三种模式ipvs、userspace、iptables三种，这里安装时使用ipvs模式（IP VirtualServer），所以需要为它加载以下的内核模块： 12345ip_vsip_vs_rrip_vs_wrrip_vs_shnf_conntrack_ipv4 在所涉及的kubernetes节点（这里仅有node1，node2），执行以下脚本 123456789cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 上述脚本创建了/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 使用 lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令，可以查看是否已经正确加载所需的内核模块。另外，还需要确保各节点已经安装了ipset软件包（一般情况系统都已经自带了） yum install ipset，为了方便查看ipvs的代理规则，最好也安装一下管理工具ipvsadm，yum install ipvsadm。如果以上前提条件不满足，则即使kube-proxy的配置开启了ipvs模式，也会退回到iptables模式。 1.3 安装docker安装docker的yum源:（已安装过且版本匹配的可以跳过） 1234yum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo 查看最新的Docker版本： 123456789101112yum list docker-ce.x86_64 --showduplicates |sort -rdocker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 @docker-ce-stabledocker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.5-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.4-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.3-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.2-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stable docker-ce.x86_64 18.06.3.ce-3.el7 docker-ce-stable Kubernetes 1.15当前支持的docker版本列表是1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09。 这里在各节点安装docker的18.09.7版本。 1234567yum makecache fastyum install -y --setopt=obsoletes=0 \ docker-ce-18.09.7-3.el7 systemctl start dockersystemctl enable docker 确认一下iptables filter表中FOWARD链的默认策略(pllicy)为ACCEPT。 12345678910111213iptables -nvLChain INPUT (policy ACCEPT 263 packets, 19209 bytes) pkts bytes target prot opt in out source destinationChain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 0 0 DOCKER-ISOLATION-STAGE-1 all -- * * 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- * docker0 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 0 0 DOCKER all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 docker0 0.0.0.0/0 0.0.0.0/0 如果有不匹配的，请使用 iptables -P FORWARD ACCEPT 修改过来 1.4 修改docker cgroup driver为systemd根据文档CRI installation中的内容，对于使用systemd作为init system的Linux的发行版，使用systemd作为docker的cgroup driver可以确保服务器节点在资源紧张的情况更加稳定，因此这里修改各个节点上docker的cgroup driver为systemd。 创建或修改/etc/docker/daemon.json(这里很重要，错了就有坑)：12345cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;EOF 重启docker并设置开机启动（没有设置开机启动初始化时会失败）： 1234systemctl restart dockerdocker info | grep CgroupCgroup Driver: systemd 2 使用kubeadm部署kubernetes2.1 安装kubeadm和kubelet下面在各节点安装kubeadm和kubelet，这里涉及访问google，需要科学上网。（我试着找过阿里云镜像地址，但是并不能用……..） 12345678910cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF 执行如下指令 1234567891011yum makecache fastyum install -y kubelet kubeadm kubectl... 已安装: kubeadm.x86_64 0:1.15.0-0 kubectl.x86_64 0:1.15.0-0 kubelet.x86_64 0:1.15.0-0 作为依赖被安装: cri-tools.x86_64 0:1.13.0-0 kubernetes-cni.x86_64 0:0.7.5-0 完毕！ 从安装结果可以看出还安装了cri-tools, kubernetes-cni的依赖 Kubernetes 1.8开始要求关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动。 关闭系统的Swap方法如下: 1swapoff -a 修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行： 1vm.swappiness=0 执行sysctl -p /etc/sysctl.d/k8s.conf使修改生效。因为这里本次用于测试两台主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。 使用kubelet的启动参数–fail-swap-on=false去掉必须关闭Swap的限制，修改/etc/sysconfig/kubelet，加入： 1KUBELET_EXTRA_ARGS=--fail-swap-on=false 2.2 使用kubeadm init初始化集群在各节点开机启动kubelet服务： 1systemctl enable kubelet.service 使用kubeadm config print init-defaults可以打印集群初始化默认的使用的配置： 1234567891011121314151617181920212223242526272829303132333435363738apiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 1.2.3.4 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: localhost.localdomain taints: - effect: NoSchedule key: node-role.kubernetes.io/master---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfigurationkubernetesVersion: v1.14.0networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125; 从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件kubeadm.yaml(新建，任意位置下都行，advertiseAddress改成你的ip即可)： 123456789101112131415apiVersion: kubeadm.k8s.io/v1beta2kind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 192.168.110.128 bindPort: 6443nodeRegistration: taints: - effect: PreferNoSchedule key: node-role.kubernetes.io/master---apiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationkubernetesVersion: v1.15.0networking: podSubnet: 10.244.0.0/16 在开始初始化集群之前可以使用kubeadm config images pull预先在各个节点上拉取所k8s需要的docker镜像。 接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap,NumCPU...[init] Using Kubernetes version: v1.15.0[preflight] Running pre-flight checks [WARNING NumCPU]: the number of available CPUs 1 is less than the required 2 [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;[certs] Generating &quot;front-proxy-ca&quot; certificate and key[certs] Generating &quot;front-proxy-client&quot; certificate and key[certs] Generating &quot;etcd/ca&quot; certificate and key[certs] Generating &quot;etcd/server&quot; certificate and key[certs] etcd/server serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.110.128 127.0.0.1 ::1][certs] Generating &quot;etcd/peer&quot; certificate and key[certs] etcd/peer serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.110.128 127.0.0.1 ::1][certs] Generating &quot;apiserver-etcd-client&quot; certificate and key[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key[certs] Generating &quot;ca&quot; certificate and key[certs] Generating &quot;apiserver&quot; certificate and key[certs] apiserver serving cert is signed for DNS names [localhost.localdomain kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.110.128][certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key[certs] Generating &quot;sa&quot; key and public key[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s[apiclient] All control plane components are healthy after 22.501928 seconds[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.15&quot; in namespace kube-system with the configuration for the kubelets in the cluster[upload-certs] Skipping phase. Please see --upload-certs[mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;[mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the taints [node-role.kubernetes.io/master:PreferNoSchedule][bootstrap-token] Using token: fbd9w8.6j1yjer52w2po3q4[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.110.130:6443 --token luxlja.y8td78jdh7immmfd \ --discovery-token-ca-cert-hash sha256:ab640c9f8a8497c23bb91454e4734a6642aaa3c6b622c0158a94a8b5ab29bb85 运行该指令时，kubernetes会检查环境是否符合要求，诸如cpu核心数不能小于2，我这里的虚拟机只分配了一个核心，所以–ignore-preflight-errors中加上了NumCPU忽略掉了，否则会启动不成功。上面记录了完成的初始化输出的内容，根据输出的内容基本上可以看出手动初始化安装一个Kubernetes集群所需要的关键步骤。 其中有以下关键内容： [kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml” [certs]生成相关的各种证书 [kubeconfig]生成相关的kubeconfig文件 [control-plane]使用/etc/kubernetes/manifests目录中的yaml文件创建apiserver、controller-manager、scheduler的静态pod [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到 下面的命令是配置常规用户如何使用kubectl访问集群：（这里必须执行，master和worker都需要） 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 最后给出了将节点加入集群的命令 12kubeadm join 192.168.110.130:6443 --token luxlja.y8td78jdh7immmfd \--discovery-token-ca-cert-hash sha256:ab640c9f8a8497c23bb91454e4734a6642aaa3c6b622c0158a94a8b5ab29bb85 查看一下集群状态，确认个组件都处于healthy状态： 1234567kubectl get cs...NAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; 集群初始化如果遇到问题，可以使用下面的命令进行清理：(没问题的直接跳过) 123456kubeadm resetifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf /var/lib/cni/ 2.3 安装Pod Network接下来安装flannel network add-on： 123456789101112131415mkdir -p ~/k8s/cd ~/k8scurl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f kube-flannel.yml...clusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/flannel createdserviceaccount/flannel createdconfigmap/kube-flannel-cfg createddaemonset.extensions/kube-flannel-ds-amd64 createddaemonset.extensions/kube-flannel-ds-arm64 createddaemonset.extensions/kube-flannel-ds-arm createddaemonset.extensions/kube-flannel-ds-ppc64le createddaemonset.extensions/kube-flannel-ds-s390x created 如果Node有多个网卡的话,需要修改kube-flannel.yml，为flanneld启动参数加上 –-iface=&lt;iface-name&gt;（kubectl apply -f kube-flannel.yml对应安装flannel，同样的，如果安装出现问题需要重装，请使用kubectl delete -f kube-flannel.yml删除后再尝试。）12345678910containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=ens33...... 检查master状态为Ready 12345kubectl get nodes...NAME STATUS ROLES AGE VERSIONlocalhost.localdomain Ready master 75m v1.15.0 使用kubectl get pod –all-namespaces -o wide确保所有的Pod都处于Running状态。(如果有其它状态的，请检查selinux关闭后需要重启) 123456789101112kubectl get pod -n kube-system...NAME READY STATUS RESTARTS AGEcoredns-5c98db65d4-f9jrh 1/1 Running 0 79mcoredns-5c98db65d4-w6cpx 1/1 Running 0 79metcd-localhost.localdomain 1/1 Running 1 78mkube-apiserver-localhost.localdomain 1/1 Running 1 78mkube-controller-manager-localhost.localdomain 1/1 Running 1 78mkube-flannel-ds-amd64-dqv67 1/1 Running 0 20mkube-proxy-c7cwv 1/1 Running 1 79mkube-scheduler-localhost.localdomain 1/1 Running 1 78m 2.4 测试集群DNS是否可用123456kubectl run curl --image=radial/busyboxplus:curl -it...kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.If you don&apos;t see a command prompt, try pressing enter.[ root@curl-5cc7b478b6-r997p:/ ]$ 上述命令就是进入容器的意思，如果命令卡住了，尝试docker exec 该镜像也可以看到效果。进入容器后执行nslookup kubernetes.default确认解析正常: 12345678nslookup kubernetes.default...Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetes.defaultAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local 2.5 向Kubernetes集群中添加Node节点下面将node2这个主机添加到Kubernetes集群中（node2不需要执行kubeadm init），在node2上执行: 12kubeadm join 192.168.110.130:6443 --token luxlja.y8td78jdh7immmfd \ --discovery-token-ca-cert-hash sha256:ab640c9f8a8497c23bb91454e4734a6642aaa3c6b622c0158a94a8b5ab29bb85 随后在node1节点上运行kubectl get nodes即可： 123456kubectl get nodes...NAME STATUS ROLES AGE VERSIONnode1 Ready master 4m36s v1.15.0node2 Ready &lt;none&gt; 69s v1.15.0 2.6 kube-proxy开启ipvs1kubectl edit cm kube-proxy -n kube-system 找到如下部分的配置内容 1234567kind: KubeProxyConfigurationmetricsBindAddress: 127.0.0.1:10249mode: &quot;ipvs&quot; #默认是空的，修改为ipvsnodePortAddresses: nulloomScoreAdj: -999portRange: &quot;&quot;resourceContainer: /kube-proxy 运行如下指令，会将之前的kube-proxy pod删除然后重新创建： 12345kubectl get pod -n kube-system | grep kube-proxy | awk &apos;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&apos;...pod &quot;kube-proxy-8xv5x&quot; deletedpod &quot;kube-proxy-skcbw&quot; deleted 查看新创建的kube-proxy：12345kubectl get pod -n kube-system | grep kube-proxy...kube-proxy-7fsrg 1/1 Running 0 3skube-proxy-k8vhm 1/1 Running 0 9s 12345678910111213kubectl logs kube-proxy-7fsrg -n kube-system...I0703 04:42:33.308289 1 server_others.go:170] Using ipvs Proxier.W0703 04:42:33.309074 1 proxier.go:401] IPVS scheduler not specified, use rr by defaultI0703 04:42:33.309831 1 server.go:534] Version: v1.15.0I0703 04:42:33.320088 1 conntrack.go:52] Setting nf_conntrack_max to 131072I0703 04:42:33.320365 1 config.go:96] Starting endpoints config controllerI0703 04:42:33.320393 1 controller_utils.go:1029] Waiting for caches to sync for endpoints config controllerI0703 04:42:33.320455 1 config.go:187] Starting service config controllerI0703 04:42:33.320470 1 controller_utils.go:1029] Waiting for caches to sync for service config controllerI0703 04:42:33.420899 1 controller_utils.go:1036] Caches are synced for endpoints config controllerI0703 04:42:33.420969 1 controller_utils.go:1036] Caches are synced for service config controller 日志中打印出了Using ipvs Proxier，说明ipvs模式已经开启。 3.移除节点如果需要从集群中移除node2这个Node执行下面的命令： 在master节点上执行： 12kubectl drain node2 --delete-local-data --force --ignore-daemonsetskubectl delete node node2 在node2上执行： 123456kubeadm resetifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf /var/lib/cni/ 在node1上执行： 1kubectl delete node node2]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>容器</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-alpine镜像安装字体库]]></title>
    <url>%2F2019%2F07%2F07%2Fdocker-alpine%E9%95%9C%E5%83%8F%E5%AE%89%E8%A3%85%E5%AD%97%E4%BD%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[记录下前不久在制作镜像时踩的一个坑。（镜像的主要功能是，发送一个图片地址和中文名称。然后将微软雅黑字体作为签字写到图片上并返回。） openjdk与oraclejdk为了减小镜像的大小，我使用了alpine镜像。开始我的dockerfile如下：12345FROM java:8-alpineUSER rootRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeCOPY target/image-sign.jar image-sign.jarENTRYPOINT ["java","-jar","image-sign.jar"] 使用docker build时候成功的制作了镜像。诶？好像看起来没什么问题啊？docker run启动容器。然后访问接口：空指针？单独运行程序没有问题的呀，怎么放到docker就出现这个问题了。仔细一查发现是因为dockerfile制作镜像时下载的openjdk的原因,openjdk是开源的，在oracle接手之后，两者的内容已经有些区别了。From java:8-alpine这一句，下载的是openjdk8，而我需要的是oraclejdk8.两者在功能上的小差别导致了这个问题。将基础镜像改为FROM anapsix/alpine-java之后就ok了 linux字体库的锅修改为oraclejdk后，镜像虽然能够正常运行了，但我通过接口转入的中文却不能正常显示:在网上翻阅到的原因都说是缺少字体，linux下是没有微软雅黑、宋体这些字体的。但我按照他们的方式却怎么也安装不成功(后来发现是我网络的问题。),最后找到的解决办法如下：1.将windows下的字体找到，复制出来。宋体为simsun.ttc,需要改后缀名为ttf，微软雅黑为msyf.tff。2.编辑dockerfile为如下即可：12345678910FROM anapsix/alpine-javaRUN echo "http://mirror.math.princeton.edu/pub/alpinelinux/v3.8/main" &gt; /etc/apk/repositories \ &amp;&amp; echo "http://mirror.math.princeton.edu/pub/alpinelinux/v3.8/community" &gt;&gt; /etc/apk/repositories \ &amp;&amp; apk add ttf-dejavu fontconfig \ &amp;&amp; rm -rf /var/cache/apk/* \ &amp;&amp; ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \ &amp;&amp; echo "Asia/Shanghai" &gt; /etc/timezone COPY target/image-sign.jar image-sign.jarCOPY ttf/* /usr/share/fonts/ENTRYPOINT ["java","-jar","image-sign.jar"] 这里做下解释，apk是alpine系统的安装工具，类似centos的yum与ubuntu系统的apt-get，前两句echo是更改apk安装的镜像地址，不清楚是资源的问题还是啥，在我没有改这个的时候。一直安装不成功。具体可见github的这个issues：apk WARNING Ignoring APKINDEX No such file or director 当然，这个办法也是在issue里找到的= =，github还是解决问题的好地方啊~，之后的apk add ttf-dejavu fontconfig是为容器安装字体库，同时删除安装包(减小镜像大小)。COPY ttf/* /usr/share/fonts/ 将从windows下拿到的字体文件放入docker镜像内，问题解决~ ps: dockerfile如果具有多个RUN指令，考虑写在同一行的话会减少镜像的体积。毕竟docker每运行一个指令都会加上一层layer。 以上です～。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>容器</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式笔记(一)]]></title>
    <url>%2F2019%2F05%2F28%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%AC%94%E8%AE%B0(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[从单机应用war包，到多机部署产生的问题：1.session跨域 session sticky session replication session 集中存储（redis等） cookie 逆向思维，由客户端来存储 2.请求转发，如何平均的请求到每一台机器上(负载均衡) Apache Nginx （http负载均衡支持很好） haproxy （tcp/ip界的老牌负载均衡器） lvs (不支持虚拟化) 由数据库读写瓶颈锁带来的，从单体数据库到读写分离产生的问题 数据库读写分离如何操作（读写分离配置） 读与写之间如何同步 （同步不及时？一致性与可用性） 数据库操作如何路由，什么操作用读库，什么操作用写库（mycat） 数据量过大的情况，如何实现高效率搜索搜索引擎也带来一个问题，如何进行数据同步？数据增量？全量同步？集群健康？ Elasticsearch solr lucene 持续性大量访问，数据库无法承受带来的问题使用缓存(redis、memcache)，降低数据库的直接大量访问，缓存集群（缓存的雪崩、击穿、穿透、数据不一致）限流、降级、熔断（高可用的保护措施） 数据库瓶颈的再延伸——数据库集群的使用 mysql的pxc集群（高可用，强一致性），replication集群 随异步处理，解耦，削峰而产生的—-&gt;消息队列 rabbitmq kafka activemq rocketmq 前后端分离，静态文件—CDN服务器的使用 应用拆分——&gt;微服务 服务注册 负载均衡 RPC远程调用 配置中心 路由规则 链路追踪]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[源码分析之LinkedList]]></title>
    <url>%2F2019%2F05%2F11%2F%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLinkedList%2F</url>
    <content type="text"><![CDATA[在leetcode中遇到了一道设计链表的题，要求中有一点是不能使用内置的LinkedList库，于是在自己实现了简易版的链表之后，找到了内置的源码来做个分析。 可以自行选择实现单向或双向链表 原题链接。首先，LinkedList是个双向链表，每个数据结点中都有两个“指针”，分别指向直接后继和直接前驱。所以，从双向链表中的任意一个结点开始，都可以很方便地访问它的前驱结点和后继结点（百科）。首先，来看看LinkedList的基础属性：12345678910111213141516171819202122232425public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable &#123; transient int size = 0; //节点个数 transient Node&lt;E&gt; first; //首节点 transient Node&lt;E&gt; last; //尾节点 public LinkedList() &#123; //初始化一个空的链表集合 &#125; public LinkedList(Collection&lt;? extends E&gt; c) &#123; //根据已有集合初始化 this(); addAll(c); //稍后分析addAll &#125; //内部类---节点 private static class Node&lt;E&gt; &#123; E item; //节点本身 Node&lt;E&gt; next; //前驱节点 Node&lt;E&gt; prev; //后继节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; &#125; 节点的添加1234567891011121314151617181920//默认的add会加到尾部public boolean add(E e) &#123; linkLast(e); return true; &#125;//指定位置添加public void add(int index, E element) &#123; checkPositionIndex(index); //检查节点是否存在 if (index == size) linkLast(element); //在节点尾部添加 else linkBefore(element, node(index)); //在指定节点前添加，node(index)会查询出位于index位置的节点&#125;public void addFirst(E e) &#123; linkFirst(e); &#125;public void addLast(E e) &#123; linkLast(e); &#125; 1.检查节点是否存在（方法很简单，判断了下size和index）:1234567private void checkPositionIndex(int index) &#123; if (!isPositionIndex(index)) //isPositionIndex throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125;private boolean isPositionIndex(int index) &#123; return index &gt;= 0 &amp;&amp; index &lt;= size; &#125; 2.在节点尾部/首部添加：12345678910111213141516171819202122232425//在首部添加节点，此方法为私有方法，实际使用时调用addFirstprivate void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; //将final类型的f节点指向首节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); //新建节点 first = newNode; //将新节点变为首节点 if (f == null) last = newNode; //链表为空，此时first = last = newNode else f.prev = newNode; //将新节点加入到链表中 size++; //节点数+1 modCount++; //modCount表示更改次数，在遍历时才会用到&#125;//在尾部添加节点，此方法没有修饰符void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++; &#125; linkFirst与linkLast类似，这里介绍简单说下linkFirst，首先将当前的首节点的引用赋值给f，然后新建一个Node节点，节点的next指向f，prev为null。将新节点变为首节点，同时判断f是否为空，如果f为空说明链表为空，此时新节点既为首节点也为尾节点，否则将f的prev指向新的节点。此时新节点就加到了链表中。3.在指定位置添加:123456789101112131415161718192021222324252627//先看下node(index)如何定位到节点Node&lt;E&gt; node(int index) &#123; //这里的移位操作size&gt;&gt;1其实就是size的一半，如果index小于size的一半，那么从前往后找，否则从后往前找，这样可以加快查找速度 if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125; &#125;//在指定节点前添加void linkBefore(E e, Node&lt;E&gt; succ) &#123; final Node&lt;E&gt; pred = succ.prev; //拿到指定节点的前一个 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); //新的节点next指向succ，pre指向pred succ.prev = newNode; if (pred == null) first = newNode; //pred为空的话，表示succ为first，将first替换为newNode else pred.next = newNode; //否则pred的next指向newNode size++; modCount++; &#125; 构造方法addAll123456789101112131415161718192021222324252627282930313233343536373839404142public boolean addAll(Collection&lt;? extends E&gt; c) &#123; return addAll(size, c); //传入当前size 和构造参数&#125;//在指定位置index的前方，添加多个节点public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; checkPositionIndex(index); Object[] a = c.toArray(); int numNew = a.length; //需要添加的元素个数 if (numNew == 0) return false; Node&lt;E&gt; pred, succ; if (index == size) &#123; //位置等于size，即在链表末尾添加，否则在链表中间添加 succ = null; pred = last; &#125; else &#123; succ = node(index); //index处的节点 pred = succ.prev; //index的上一个节点 &#125; for (Object o : a) &#123; @SuppressWarnings("unchecked") E e = (E) o; Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); //创建新节点 if (pred == null) first = newNode; //初始化第一个节点 else pred.next = newNode; //链接新节点 pred = newNode; &#125; if (succ == null) &#123; last = pred; //在尾部插入，那么循环结束后，pred就是尾节点 &#125; else &#123; pred.next = succ; //否则，需要将pred的next指向succ节点 succ.prev = pred; &#125; size += numNew; //修改节点个数 modCount++; return true; &#125; 节点的删除删除操作有多种方式，但都是基于unlink方法实现的，这里就简单写一种吧(偷懒ing):123456789101112131415161718192021222324252627282930public E remove(int index) &#123; checkElementIndex(index); //检查是否存在元素 return unlink(node(index)); //执行unlink&#125;//unlink操作E unlink(Node&lt;E&gt; x) &#123; final E element = x.item; //需要删除的节点赋给final的element常量 final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; //需要删除的节点为first，那么删除后它next变为first &#125; else &#123; prev.next = next; //前后节点相连，跳过当前节点 x.prev = null; //把需要删除节点的pre变为null &#125; if (next == null) &#123; last = prev; //需要删除的节点为last，删除后它的pre变为last &#125; else &#123; next.prev = prev; //前后节点相连，跳过当前节点 x.next = null; //把需要删除节点的next也变为null &#125; x.item = null; //将节点的item也变为null，帮助gc回收 size--; //节点数-1 modCount++; return element; //返回element&#125; 节点的查找12345//get方法还是调用的node(index)进行定位，返回节点的itempublic E get(int index) &#123; checkElementIndex(index); return node(index).item; &#125; 到这里，LinkedList的基础方法就分析完了，其余的高级方法诸如push、pop、peek、poll等等，都是基于这些基本方法完成的。（java中的栈是基于Vector实现的，每个方法都有Syncronized修饰，所以在1.5之后LinkedList添加了用于实现无锁栈的方法）123456789101112131415public void push(E e) &#123; addFirst(e); &#125;public E pop() &#123; return removeFirst(); &#125;//前两个为空会抛出异常，peek和poll不会public E peek() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : f.item; &#125;public E poll() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : unlinkFirst(f); &#125; 补充： 在源码中可以看到transient和final关键字的使用，这里也把两者的作用记下。 transient的作用： 阻止实例中那些用此关键字修饰的变量序列化；当对象被反序列化时，被transient修饰的变量值不会被持久化和恢复。transient只能修饰变量，不能修饰类和方法。 final关键字的一些总结： 1.对于一个变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能修改，如果是引用类型的变量，则在对其初始化之后便不能再让其指向另外一个对象。 2.当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会隐式地指定为final方法。 3.使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。类中所有的private方法都隐式地指定为final。在早期的java版本中，会将final方法转为内嵌调用。但如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现版本已经不需要使用final方法进行这些优化了）。]]></content>
      <categories>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的live2d配置]]></title>
    <url>%2F2019%2F04%2F27%2F%E6%88%91%E7%9A%84live2d%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[给hexo加上live2d只能说是自己作为死宅的本能反应吧= =，在别人的网站上看过之后总是觉得羨ましい，然后到处找文章想给自己也加上，本来很简单的事情还用了大半天(I very vegetables)，这次把步骤写上来。也算做个备份吧，指不定那天忘了呢。 hexo的官方插件hexo的官方有提供名为hexo-helper-live2d的插件使用如下安装命令就可以安装 npm install –save hexo-helper-live2d 随后在Hexo的_config.yml文件中添加如下配置(据说是可以配在主题的_config.yml中的，但是我没试过呢orz).示例: 12345678910111213141516live2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false debug: false model: use: live2d-widget-model-wanko //启用的模型目录 display: position: right width: 150 height: 300 mobile: show: true 这样一个默认的模型就配好了，默认是这样的: 添加互动：默认的模型只有很简单的动画，在别人网站里看到的都是能互动对话的，经过查阅找到了大佬的教程把萌萌哒的看板娘抱回家。 (到这里需要把之前配置文件中的live2d.enable修改为false)根据步骤来，jquery在主题中是已经存在的直接跳过，将项目下载下来，将autoload.js、 live2d.min.js、waifu-tips.js、waifu-tips.json、waifu.css这几个文件放入到next主题中/source/js/src目录下，也可以将json和css文件放到对于的目录下，但是需要改动下autoload中的路径，我这里偷懒直接放到一起了orz &lt;script src=&quot;/js/src/autoload.js&quot;&gt;&lt;/script&gt; 将上面这句放入到主题的/layout/_layout.swing中body标签的末尾，autoload.js的内容如下：123456789101112131415161718192021222324252627282930//开始加斜杠和不加是完全不同的路径,这里改为了我放置的相对路径const live2d_path = "/js/src/";//const live2d_path = "./";$("&lt;link&gt;").attr(&#123;href: live2d_path + "waifu.css", rel: "stylesheet", type: "text/css"&#125;).appendTo("head");//waifu.css的绝对路径$.ajax(&#123; url: live2d_path + "live2d.min.js", dataType: "script", cache: true, async: false&#125;);//live2d.min.js的绝对路径$.ajax(&#123; url: live2d_path + "waifu-tips.js", dataType: "script", cache: true, async: false&#125;);//waifu-tips.js的绝对路径//初始化看板娘，会自动加载指定目录下的waifu-tips.json$(window).on("load", function() &#123; initWidget(live2d_path + "waifu-tips.json", "https://live2d.fghrsh.net/api");&#125;);//initWidget第一个参数为waifu-tips.json的绝对路径//第二个参数为api地址（无需修改）//api后端可自行搭建，参考https://github.com/fghrsh/live2d_api 然后打开waifu-tips.js，在方法initWidget中将下面这句话移到方法的最前面，这样可以在关闭live2d后，刷新时重新加载。 localStorage.removeItem(&quot;waifu-display&quot;); 添加拖动上述操作完成之后却发现一个问题，模型位置是固定的，不能拖动–。进一步了解之后，自定义的编写了如下的拖动方法(放在了waifu-tips.js中)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 绘制图片坐标 var X=0; var Y=0;// js部分 var divObj=document.getElementById("waifu"); var moveFlag=false;//区别moueseup与click的标志 var clickFlag=false;// 拖拽函数 divObj.onmousedown=function(e)&#123; moveFlag=true; clickFlag=true; var clickEvent=window.event||e; var mwidth=clickEvent.clientX-divObj.offsetLeft; var mheight=clickEvent.clientY-divObj.offsetTop; document.onmousemove=function(e)&#123; clickFlag=false; var moveEvent=window.event||e; if(moveFlag)&#123; divObj.style.left=moveEvent.clientX-mwidth+"px"; divObj.style.top=moveEvent.clientY-mheight+"px";//// 将鼠标坐标传给Canvas中的图像 X=moveEvent.clientX-mwidth; Y=moveEvent.clientY-mheight;//// 下面四个条件为限制div以及图像的活动边界 if(moveEvent.clientX&lt;=mwidth)&#123; divObj.style.left=0+"px"; X=0; &#125; if(parseInt(divObj.style.left)+divObj.offsetWidth &gt;=innerWidth)&#123; divObj.style.left=innerWidth - divObj.offsetWidth+"px"; X=innerWidth - divObj.offsetWidth; &#125; if(moveEvent.clientY&lt;=mheight)&#123; divObj.style.top=0+"px"; Y=0; &#125; if(parseInt(divObj.style.top)+divObj.offsetHeight&gt;=innerHeight)&#123; divObj.style.top=innerHeight-divObj.offsetHeight+"px"; Y=innerHeight-divObj.offsetHeight; &#125; divObj.onmouseup=function()&#123; moveFlag=false; &#125; &#125; &#125; &#125;; 整个过程就是这样了，以上です～(｡-_-｡)。tips： 在waifu-tips.js中waifu-tool里有几个写好的模块，可以自定义添加或删除，每个span对应一个功能initmodel方法里有默认的加载模型，可以将随机更换注释掉，选择喜欢的固定模型waifu-tips.json中包含了触发条件（选择器的事件）和触发时显示的文字，也可以自定义]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>另一个次元</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池]]></title>
    <url>%2F2019%2F04%2F22%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%B7%A5%E4%BD%9C%E6%96%B9%E5%BC%8F%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[为什么要使用线程池线程池是并发场景中比较常见的运用，几乎所有的异步或并发执行任务的程序都可以使用线程池。在开发中使用线程池能带来以下好处。 降低资源消耗。重复利用已创建的线程，降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不用等待线程的创建，直接执行。 提高线程的可管理性。线程是稀缺资源，不会无限制地创建。不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。 线程池的工作原理线程池的创建依赖于ThreadPoolExecutor，它的构造函数如下所示：1234567891011public ThreadPoolExecutor(int corePoolSize, //核心线程数量 int maximumPoolSize, //最大线程数 long keepAliveTime, //超时时间,超出核心线程数量以外的线程空余存活时间 TimeUnit unit, //存活时间单位 BlockingQueue&lt;Runnable&gt; workQueue, //保存执行任务的队列 ThreadFactory threadFactory,//创建新线程使用的工厂RejectedExecutionHandler handler //当任务无法执行的时候的处理方式) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler);&#125; 创建线程池所需要的参数: corePoolSize(核心线程数量): 当提交一个任务到线程池中，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行任务也会创建新的线程，直到池中的线程数达到corePoolSize的大小就不再创建。 workQueue(工作/任务队列)：用于保存等待执行的任务的阻塞队列。 maximumPoolSize(最大线程数): 线程池所允许创建的最大线程数量，如果工作队列满了，并且已创建的的线程数小于最大线程数，此时线程池会临时创建新的线程执行任务。 keepAliveTime（线程活动保持时间）：线程池的工作线程空闲后，保持存活的时间，超过时间会被回收。 unit: 线程保持活动时间的单位。 threadFactory: 用于设置创建线程的工厂。 handler(饱和处理器)：当工作队列为有界队列，并且池中的线程数量已经达到了最大线程数，此时新提交的任务就会由handler进行饱和处理，抛出异常。 当提交一个新任务到线程池中，线程池的处理流程如下： 判断线程是否已经达到核心线程数，如果当前池中线程数少于核心线程数，创建一个新线程，否则进入下一个流程。 判断工作队列是否已满，如果未满，则将任务放入队列中，如果队列已满，则进行下一个流程。 判断当前池中线程数是否已达到最大线程数，如果未达到，则创建新的线程并执行任务，如果已达到最大线程数，则任务会被拒绝。 当其他的线程执行完任务时，会进入空闲状态，如果队列中有任务，会取出来执行，当队列为空之后，超过空闲存活时间的队列会被回收。流程图： execute方法流程分析1234567891011121314151617181920public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123;//1.当前池中线程比核心数少，新建一个线程执行任务 if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123;//2.核心池已满，但任务队列未满，添加到队列中 int recheck = ctl.get(); //任务成功添加到队列以后，再次检查是否需要添加新的线程，因为已存在的线程可能被销毁了 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); //如果线程池处于非运行状态，并且把当前的任务从任务队列中移除成功，则拒绝该任务 else if (workerCountOf(recheck) == 0) addWorker(null, false);//如果之前的线程已被销毁完，新建一个线程 &#125; else if (!addWorker(command, false)) //3.核心池已满，队列已满，试着创建一个新线程 reject(command); //如果创建新线程失败了，说明线程池被关闭或者线程池完全满了，拒绝任务&#125; submit和execute的区别向一个线程池提交任务，可以使用submit和execute，这两者有什么区别呢？ execute只能接受Runnable类型的任务，execute没有返回值 submit不管是Runnable还是Callable类型的任务都可以接受，但是Runnable返回值均为void，所以使用Future的get()获得的还是null 不得不说- -，记录这篇文章的时候。满脑子都是当初在滴p科技面试时没答上来的尴尬(2333),也算是巩固了一遍知识吧~~放上我的老婆= =]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql下获取某月的每一天]]></title>
    <url>%2F2019%2F04%2F22%2Fmysql%E4%B8%8B%E8%8E%B7%E5%8F%96%E6%9F%90%E6%9C%88%E7%9A%84%E6%AF%8F%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[记得以前自己写数据统计接口时，遇到一个根据选择的月份，查询出每天的数据的要求，由于每个月份的天数不同，不能按照固定天数来查。经过查阅之后找到了如下的方式： 使用union和日期函数构造一个左表12345678910111213SELECT ADDDATE(y.first, x.d - 1) as dd FROM (SELECT 1 AS d UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4 UNION ALL SELECT 5 UNION ALL SELECT 6 UNION ALL SELECT 7 UNION ALL SELECT 8 UNION ALL SELECT 9 UNION ALL SELECT 10 UNION ALL SELECT 11 UNION ALL SELECT 12 UNION ALL SELECT 13 UNION ALL SELECT 14 UNION ALL SELECT 15 UNION ALL SELECT 16 UNION ALL SELECT 17 UNION ALL SELECT 18 UNION ALL SELECT 19 UNION ALL SELECT 20 UNION ALL SELECT 21 UNION ALL SELECT 22 UNION ALL SELECT 23 UNION ALL SELECT 24 UNION ALL SELECT 25 UNION ALL SELECT 26 UNION ALL SELECT 27 UNION ALL SELECT 28 UNION ALL SELECT 29 UNION ALL SELECT 30 UNION ALL SELECT 31 ) x, (SELECT CONCAT("2019-02",'-01') as FIRST, DAY(LAST_DAY(STR_TO_DATE("2019-02",'%Y-%m'))) AS last) y WHERE x.d &lt;= y.last 结果如下:之后只需将上面的结果集与需要的结果左连接，进行聚合就拿到结果啦~近期准备慢慢的把有道云里的笔记搬上来占个位= =]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用设计模式---单例模式]]></title>
    <url>%2F2019%2F04%2F14%2F%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F---%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[单例模式简单介绍单例模式确保某一个类只有一个实例，而且自行实例化并向整个系统提供这个实例，这个类成为单例类，它提供全局访问的方法。单例模式的要点有三个：一是某个类只能有一个实例；二是它必须自行创建这个实例；三是它必须自行向整个系统提供这个实例。 模式结构图 单例模式需要注意事项1.单例类的构造函数私有2.提供一个自身的静态私有成员变量3.提供一个公有的静态工厂方法 单例模式实例这里模拟实现一个居民身份证唯一的单例场景。123456789101112131415161718192021222324// 单例类如下public class IdCardNo &#123; private static IdCardNo instance = null; private String no; private IdCardNo() &#123; &#125; public static IdCardNo getInstance() &#123; if (instance == null) &#123; instance = new IdCardNo(); instance.setNo("No5000011113333"); &#125; return instance; &#125; public String getNo() &#123; return no; &#125; private void setNo(String no) &#123; this.no = no; &#125; 单例的多种写法上述场景中使用的是懒汉式写法，单例模式还有如下的几种写法饿汉式：1234567891011public class EagerSingleton &#123; private static EagerSingleton instance = new EagerSingleton(); private EagerSingleton() &#123; &#125; public static EagerSingleton getInstance() &#123; return instance; &#125; &#125; 饿汉式的写法可以保证线程安全，但从资源利用率角度来考虑，比懒汉式写法稍差。但懒汉式存在线程安全问题，所以接下来考虑多个线程同时首次引用单例的访问限制问题。双重检测的单例：123456789101112131415161718public class Singleton &#123; //volatile禁止指令重排序，保证可见性 private static volatile Singleton instance; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; &#125; 双重检测虽然解决了多线程的访问限制问题，但这个写法看起来着实不美观。那么我们还有没有别的写法呢？答案是有的。基于枚举的方式：12345678910111213public enum SingletonEnum &#123; INSTANCE; private Singleton instance = null; private SingletonEnum() &#123; instance = new Singleton(); &#125; public Singleton getInstance() &#123; return instance; &#125; &#125; 单元素的枚举类可以保证单例的线程安全、序列化，除单元素枚举外，还有使用java内部类实现的方式。基于内部类实现单例：1234567891011121314151617181920212223242526272829303132public class Singleton implements Serializable &#123; private Singleton() &#123; &#125; private static class SingletonHandler &#123; private static Singleton instance = new Singleton(); &#125; public static Singleton getInstance() &#123; return SingletonHandler.instance; &#125; private Object readResolve()&#123; System.out.println("read resolve"); return SingletonHandler.instance; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; Singleton instance = Singleton.getInstance(); File file = new File("ser.out"); ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file)); oos.writeObject(instance); oos.close(); ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file)); Singleton oIstance = (Singleton)ois.readObject(); ois.close(); System.out.println(oIstance == instance); &#125; &#125; 使用静态内部类的优点是：外部类加载时并不需要立即加载内部类，即当Singletonle被加载时，并不需要去加载SingletonHandler，只有当getInstance()方法第一次被调用时，才会去初始化SingletonHandler,同时初始化该类的静态变量instance,在确保线程安全的同时也延迟了单例的实例化. 总结一个类模板，在整个系统中只允许产生一个实例叫做单例。单例有多种写法：懒汉式、饿汉式、双重检查、枚举、内部类。 饿汉式不管用不用先创建出来，保证线程安全。 懒汉式延迟加载，有效利用资源不保证线程安全。 双重检测方式保证了懒汉式的线程安全问题。 单元素枚举可以同时保证线程安全和序列化。 内部类使用了jvm的类加载机制来保证线程安全和懒加载。 序列化和反序列化保证单例需要重写类的readResolve()方法]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我来了，我的博客]]></title>
    <url>%2F2018%2F09%2F04%2F%E6%88%91%E6%9D%A5%E4%BA%86%EF%BC%8C%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[很久之前，在学习github的使用时就有了搭建这个个人博客的想法，虽说按照教程，早早地就已经搭建好了这个博客， 但是随着毕业季、入职工作等一系列的事情，也没有好好的静下心来整理。现如今工作也稳定下来了，一年多的时间不长不短，也是时候总结一下自己了。从小到大没有写日志习惯的我，估计写出来的东西，也只有自己能看看吧（笑），权当做给自己做个笔记，记录些工作中和生活中的小事吧。现在，第一步，先给我的hexo换个主题吧~ 第一次用markdown，语法还是挺奇怪的，不太习惯（雾）]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>开始</tag>
      </tags>
  </entry>
</search>
